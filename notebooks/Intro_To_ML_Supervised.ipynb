{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Machine Learning - Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lab we explored the 2015 King's County, Washington home sale data and prepared it for modeling.  Now we will actually use this data to test a series of algorithms, assess their performance, and select the best one.  Since we are trying to predict the sale price, this will be a regression problem.  We will fit linear, nearest neighbors, and random forest models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's first read in the data using the ``.read_csv()`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the data is read-in correctly and what you think it should be using ``.head()`` and ``.describe()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are all of the columns you want accounted for?  Are there any extras you should drop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's separate our target column (``price``) from the features.  We'll call the target ``y`` and the features ``X``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = \n",
    "X = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now split our data into a training set to develop our model and a test set that is unseen by the model against which we can test performance.  ``.train_test_split()`` is an Sci-kit Learn function that makes it easy.  We set the fraction of data we want to withhold for testing using the ``test_size`` parameter.  A value of 0.2-0.33 is usually good to start with, but once we have all of our models built, come back and change this around to see if your results are sensitive to it.  To make sure your data is randomized, shuffle it.  It is also easy to check this by plotting ``y``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our split data, we can fit our models and calculate performance metrics.\n",
    "\n",
    "Start with a linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_model = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's make predictions on the unseen test data.  We will compare this to y_test.  Keep in mind that y_test is a series, so we will need to get the values using ``.values``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate some metrics.  The two we are most interested in are mean absolute error (MAE) and mean squared error (MSE).  Print out their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print('MAE:  ', )\n",
    "print('r2:  ', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit the data with a k-nearest neighbors regressor.  Remember, this model simply finds the nearest k points and makes the prediction from a weighted average.  We'll follow the same procedure of fitting the data, making an out of sample prediction and then calculating the same metrics.  Change k using the ``n_neighbors`` parameter to see how that impacts your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "nn_model = \n",
    "\n",
    "print('MAE:  ', )\n",
    "print('r2:  ', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last model we will fit is a random forest regressor.  Random forest models are very popular with practioners because they do a great job of fitting the nuances of your data, especially in large datasets like we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "predictions = rf_model.predict(X_test)\n",
    "\n",
    "print('MAE:  ',  )\n",
    "print('r2:  ', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final exercise think a bit about our scoring metrics.  They each provide different insights into the performance of the model - what are they telling us and how can they be biased?  In particular, let's think about MAE and if it is the best metric.  Some of these values are quite high when you considering we found the median sale price to be $450,000.  What might be a better metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Sci-kit Learn we've been able to get these complex models working on our data very easily.  This greatly improves our workflow, and allows us to spend less time getting the code running and more time actually doing data science.  If you've finished early, go back and \"turn the knobs\" of the algorithms to see if you can improve the accuracy of your models.  Especially with the random forest model, there are a lot of parameters you can tweak.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
