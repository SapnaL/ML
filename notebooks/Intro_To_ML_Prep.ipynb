{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Machine Learning - Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first machine learning project we will try to predict the sale price of homes to develop a better estimate of their value.  A great dataset to use is the King's County home sale data.  This dataset contains historic home sale data for King's County in Washington state, including the city of Seattle.  Over the course of these labs we will prepare the data (including cleaning and filtering), fit it with several supervised regression models, and then fit it with unsupervised models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several things need to be done to prepare the data.  First, we need to explore the dataset to learn what it contains, a bit about its characteristics, and gain a general sense of understanding of the feature statistics.  Second, we need to determine if there are any incorrect or missing values, and correct them.  Finally, we need to standardize the features so that are not on disparate scales.\n",
    "\n",
    "To begin, let's import pandas for data manipulation, and Matplotlib and Seaborn for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/kc_house_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab the first few rows of the data to see the types of data contained in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also print off a list of the data columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ``.describe``, get summary statistics for every data column.  Do these values make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean price is informative, but it can be skewed by extreme values.  What is the median?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are any extremely odd entries that you consider to be bad data, identify the data rows, investigate them, and decide how you want to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't seen any Null values so far, but as we previously saw in the Pandas lab, just a few can be buried in a big dataset.  Using the same methods we previously did, count the number of Null values in each column.  You may need to refer back to your labs on Pandas.  If you find any Null values, remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly write a paragraph summarizing the type of data in our dataset, the unique datatypes within it, and the scales of the data in each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes about data types:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a general sense of the type of data we have, let's dig into it a bit more.  Using the visualization techniques and tools we covered, make a few plots to understand how the data is distributed and how the variables are related.  If you find anything you think could be important to remember for modeling, make note of it below.\n",
    "\n",
    "Since we want to predict price, it is important to understand how the other variables are correlated with it.  Also, we need to know how the home sale price and the feature variables are distributed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Notes about data relationships:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a good idea for what data columns may be important and the data types they contain, we can consider which ones need to be dropped or augmented.  Which columns are effectively duplicates and strongly correlated?  The ``.corr`` method and ``heatmap`` plot from Seaborn may be helpful for the latter task.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a data column with a simple true/false flag for if the house has been renovated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['renovated'] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a column to account for renovations, let's drop columns that we don't want to include in our modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final preparation step use the ``MinMaxScaler`` function from Sci-kit Learn to scale the data.  You don't want to scale the prediction column, so make a list of the columns you don't want to scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "keep_cols = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the cleaned data to a ``.csv`` file using the ``.to_csv()`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
